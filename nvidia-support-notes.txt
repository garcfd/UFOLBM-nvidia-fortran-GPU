
baseline with only collapse(3)
!$acc parallel loop collapse(3) present(phi)

(00000000)
10001 phi = 0.5369182
10001 phi = 0.5370653   
10001 phi = 0.5372299
10001 phi = 0.8297331 serial
10001 phi = 0.8297331 multicore 
real    0m5.852s
real    0m5.238s
real    0m3.717s 
real   4m54.498s serial
real   2m27.462s multicore

You can try other schedules to see if things improve, but would expect a collapse(3) to be optimal here. 
Though it’s easy to experiment. Just a few examples of schedules to try:



(11111111)
10001 phi = 0.5335192 
10001 phi = 0.5329394 
real    0m5.221s
real    0m3.953s
Collapse the outer two loops and schedule across the gangs (CUDA Blocks). 
Distribute the inner loop across the vectors (CUDA thread x-dimension) within each gang

!$acc parallel loop gang collapse(2) present(phi)
        do k = 2, kmm
        do j = 2, jmm
!$acc loop vector
        do i = 2, imm



(22222222)
10001 phi = 0.5901579 
10001 phi = 0.5902423  
real    0m5.975s
real    0m5.872s

Gang outer, collapse the inner vector loops
!$acc parallel loop gang present(phi)
        do k = 2, kmm
!$acc loop vector collapse(2)
        do j = 2, jmm
        do i = 2, imm



(33333333)
10001 phi = 0.5323855 
10001 phi = 0.5326202
real    0m5.658s
real    0m4.562s
Gang outer, use workers (CUDA Thread y-dimension) on the middle loop, vector inner
!$acc parallel loop gang present(phi)
        do k = 2, kmm
!$acc loop worker
        do j = 2, jmm
!$acc loop vector
        do i = 2, imm



(44444444)
10001 phi =    0.5182228 
10001 phi =    0.5181741 
real    0m8.313s
real    0m8.164s
Tile the inner loops. Similar to #3, but with specific sizes for the x and y thread dimensions
!$acc parallel loop gang present(phi)
        do k = 2, kmm
!$acc loop tile(16,16)    
        do j = 2, jmm
        do i = 2, imm



(55555555)
10001 phi = 0.5371951  
10001 phi = 0.5371848 
real    0m4.439s  <<<<<<<< gpu fastest
real    0m4.775s
real    4m49.822s <<<<<<<< serial

Let the compiler decide:
!$acc kernels loop independent present(phi)
        do k = 2, kmm
!$acc loop independent
        do j = 2, jmm
!$acc loop independent
        do i = 2, imm
Like it would do a Gang, Gang-vector, Vector schedule, but to see what it actually does, and the flag “-Minfo=accel” and the schedule will be shown in the feedback messages.

There’s other permutations to try, but hopefully this gives you ideas.



(66666666)
10001 phi = 0.5906206
10001 phi = 0.5909128  
real    0m4.554s
real    0m6.290s
real    0m4.289s
Oh, I should mention you can combine gang, worker, and vector on the same loop. For example:
!$acc parallel loop gang present(phi)
        do k = 2, kmm
!$acc loop worker vector collapse(2)
        do j = 2, jmm
        do i = 2, imm
This collapses the inner loop and the creates a strip-mine loop (i.e. a loop sized to the vector length).
There’s also the “vector_length”, “num_workers”, and “num_gangs” clauses if you want to override the default sizes.



MatColgrove
Moderator

